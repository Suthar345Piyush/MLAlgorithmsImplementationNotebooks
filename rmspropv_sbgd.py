# -*- coding: utf-8 -*-
"""RMSPropv/sBGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kvu_mDtZsKxBn2muZUJ-NqP5HoTlB1sr
"""

# code implementation of Rms props vs batch gd

import numpy as np
import matplotlib.pyplot as plt

# defining the quadratic loss function

def quadratic_loss(x , y):
    return x**2 + 10 * y**2

# quadratic gradient function

def quadratic_grad(x , y):
    dx = 2 * x
    dy = 20 * y
    return np.array([dx , dy])

# gradient descent function

def gradient_descent(grad_func , lr , epochs , start_point):
    x , y = start_point
    path = [(x , y)]
    losses = [quadratic_loss(x , y)]



    for _ in range(epochs):
        grad = grad_func(x , y)
        x -= lr * grad[0]
        y -= lr * grad[1]
        path.append((x , y))
        losses.append(quadratic_loss(x , y))

    return np.array(path) , losses


# function for rms prop

def rmsprop_optimizer(grad_func , lr , beta , epsilon , epochs , start_point):
    x , y = start_point
    Eg2 = np.array([0.0 , 0.0])    # moving average of squared gradients
    path = [(x , y)]
    losses = [quadratic_loss(x , y)]


    for _ in range(epochs):
        grad = grad_func(x , y)    # compute the gradients
        Eg2 = beta * Eg2 + (1 - beta) * (grad ** 2)  # update moving average


        x -= lr * grad[0]  / (np.sqrt(Eg2[0]) + epsilon)   # update x
        y -= lr * grad[1] / (np.sqrt(Eg2[1]) + epsilon)  # update y


        path.append((x , y))
        losses.append(quadratic_loss(x , y))


    return np.array(path) , losses

# plotting the paths

def plot_paths(function , paths , labels , title):
    X , Y = np.meshgrid(np.linspace(-2 , 2 , 400) , np.linspace(-2 , 2 , 400))
    Z = function(X , Y)


    plt.figure(figsize = (8 , 6))
    plt.contour(X , Y , Z , levels=50 , cmap='jet')



    for path , label in zip(paths , labels):
        plt.plot(path[:, 0] , path[:, 1] , label=label)
        plt.scatter(path[0,0] , path[0,1] , color='green' , label='Start')
        plt.scatter(path[-1 , 0] , path[-1,1] , color='red' , label='End')



    plt.title(title)
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()
    plt.show()

def plot_losses(losses , labels , title):
    plt.figure(figsize=(8 , 6))


    for loss , label in zip(losses , labels):
        plt.plot(loss , label=label)


    plt.title(title)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

# defining the parameters of the values

lr_gd = 0.1   # learning rate for gd
lr_rmsprop = 0.1   # learning rate for rmsprop
beta = 0.9  # decay rate for rms prop
epsilon = 1e-8   # small constant for rmsprop
epochs = 100
start_point = (1.5 , 1.5)    # initial far from the minimum



path_gd , losses_gd = gradient_descent(quadratic_grad , lr_gd , epochs , start_point)

path_rmsprop , losses_rmsprop = rmsprop_optimizer(quadratic_grad , lr_rmsprop , beta , epsilon , epochs , start_point)




plot_paths(quadratic_loss , [path_gd , path_rmsprop] , ["Gradient Descent" , "RMSProp"] , "Loss vs Epochs: GD vs RMSProp")