# -*- coding: utf-8 -*-
"""AdamOptimizationVsMomentum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rEaAEJA9BOe6xBNNXJ7fSpGI_EIw8yzF
"""

# adam optimization comparision with the momentum

import numpy as np
import matplotlib.pyplot as plt

# defining a quadratic loss function
# loss function -> f(x , y) = x^2 + 10*y^2 (steep valley along y-axis)

def quadratic_loss(x , y):
    return x**2 + 10 * y**2

#gradient of the loss function
def quadratic_grad(x , y):
    dx = 2 * x
    dy = 20 * y
    return np.array([dx , dy])

# Momentum Optimizer

def momentum_optimizer(grad_func , lr , beta , epochs , start_point):
    x , y = start_point
    v = np.array([0.0 , 0.0])   # initialize the velocity

    path = [(x , y)]
    losses = [quadratic_loss(x , y)]


    for _ in range(epochs):
        grad = grad_func(x , y)
        v = beta * v + (1 - beta) * grad   # update the velocity

        x -= lr * v[0]  # update the x
        y -= lr * v[1]  # update the  y



        path.append((x , y))
        losses.append(quadratic_loss(x , y))

    return np.array(path) , losses

# adam optimizer

def adam_optimizer(grad_func , lr  , beta1 , beta2 , epsilon , epochs , start_point):
    x , y = start_point
    m = np.array([0.0  , 0.0])  # first moment (momentum)
    v = np.array([0.0 , 0.0])  # second moment (rms prop)

    path = [(x , y)]
    losses = [quadratic_loss(x , y)]


    for t in range(1 , epochs + 1):
        grad = grad_func(x , y)  # compute gradients

        # update biased first moment estimate
        m = beta1 * m + (1 - beta1) * grad

        #update biased second moment estimate
        v = beta2 * v + (1 - beta2) * (grad**2)

        # bias correction

        m_hat = m / (1 - beta1**t)
        v_hat = v / (1 - beta2**t)

        # upadating the parameters

        x -= lr * m_hat[0] / (np.sqrt(v_hat[0]) + epsilon)
        y -= lr * m_hat[1] / (np.sqrt(v_hat[1]) + epsilon)

        path.append((x , y))
        losses.append(quadratic_loss(x , y))

    return np.array(path) , losses

#visulaization of paths

def plot_paths(function , paths , labels , title):
    X , Y = np.meshgrid(np.linspace(-2 , 2 , 400) , np.linspace(-2 , 2 , 400))
    Z = function(X , Y)

    plt.figure(figsize=(8 , 6))
    plt.contour(X , Y , Z , levels=50 , cmap='jet')


    for path , label in zip(paths , labels):
        plt.plot(path[:, 0], path[:, 1] , label=label)
        plt.scatter(path[0 , 0] , path[0 , 1] , color="green" , label="Start")
        plt.scatter(path[-1,0] , path[-1 , 1] , color='red' , label="End")


    plt.title(title)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend()
    plt.show()

# upadating the parameters

lr_momentum = 0.1
lr_adam = 0.1
beta1 = 0.9
beta2 = 0.999
beta_momentum = 0.9
epsilon = 1e-8
epochs = 100
start_point = (1.5 , 1.5)

# running the optmization

path_momentum , losses_momentum = momentum_optimizer(quadratic_grad , lr_momentum , beta_momentum , epochs , start_point)

path_adam , losses_adam = adam_optimizer(quadratic_grad , lr_adam , beta1 , beta2 , epsilon ,epochs , start_point)

# plotting only the path

plot_paths(quadratic_loss , [path_momentum , path_adam] , ["Momentum" , "Adam"] , "Optimization Paths: Momentum vs Adam")